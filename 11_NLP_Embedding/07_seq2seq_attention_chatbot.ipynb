{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다. seq2seq는 encoder의 마지막 hidden state를 context로 받은 뒤 그것을 이용해 모든 출력 단어들을 생성하므로 그 중요도에 대한 반영이 안된다.\n",
    "\n",
    "## Attention Mechanism 아이디어\n",
    "-  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고 하자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 계산하는 방식에 따라 다양한 attention 기법이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d5b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    with open(\"data/chatbot_data.csv\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(res.text)\n",
    "else:\n",
    "    print(f\"불러오지 못함: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6fe689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A\n",
       "0           12시 땡!   하루가 또 가네요.\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "4          PPL 심하네   눈살이 찌푸려지죠."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.drop(columns='label', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d23c37-f609-411b-aba2-17b081259cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 11823, 11823)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_texts = df['Q']\n",
    "answer_texts = df['A']\n",
    "all_texts = list(question_texts + \" \"+answer_texts) # Q + A -> vocab 생성을 위함.\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## Tokenizer 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7704dca6-ec9e-48a7-8ad2-495c08aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10_000\n",
    "min_frequency = 5 \n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##',\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"] \n",
    "    # [SOS]: 문장의 시작을 의미하는 토큰. [EOS]: 문장이 끝난 것을 표시.\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0097d1d-7d62-4c81-9c09-db61741ffefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 어휘수: 7044\n"
     ]
    }
   ],
   "source": [
    "print(\"총 어휘수:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fac5977-edbe-4c64-b0dc-0c1ee0bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"saved_models/chatbot_attn\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "vocab_path = os.path.join(dir_path, \"chatbot_attn_bpe.json\")\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=2, max_length=20):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\",   \"답2\",   ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=2: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=20:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "        # 각각의 질문, 답변 토큰 ID들을 저장할 list\n",
    "        self.question_sequences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_token = self.__process_sequence(q)\n",
    "            a_token = self.__process_sequence(a)\n",
    "            # 질문, 답변 토큰의 개수가 min_length보다 클 경우에만 list에 추가.\n",
    "            \n",
    "            if len(q_token) > min_length and len(a_token) > min_length:\n",
    "                self.question_sequences.append(q_token)\n",
    "                self.answer_sequences.append(a_token)\n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        질문/답변 토큰 리스트 맨 뒤에 문장의 끝을 표시하는 [EOS] 토큰 추가. \n",
    "        [EOS] Token을 붙이고 max_length 보다 토큰수가 많으면 안된다.\n",
    "        Args:\n",
    "            token_sequence (list[str]) - EOS 토큰을 추가할 문서 token sequence\n",
    "        \"\"\"\n",
    "        token_id_list = token_sequence[:self.max_length-1]\n",
    "        token_id_list.append(self.EOS)\n",
    "\n",
    "        return token_id_list\n",
    "\n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 encoding 한 뒤 [EOS] token을 추가한 token_id 리스트(list)를 생성 해서 반환한다.\n",
    "        Args:\n",
    "            text (str) - token id 리스트로 변환할 대상 String.\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_sequences)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        # embeddding vector 입력 -> int64\n",
    "        # batch(1) 축 추가 / 한 문장씩 사용 \n",
    "        # unsqueeze(1) -> [1,2,3,4] -> [[1],[2],[3],[4]]\n",
    "        q = torch.tensor(self.question_sequences[index], dtype=torch.int64).unsqueeze(1)\n",
    "        a = torch.tensor(self.answer_sequences[index], dtype=torch.int64).unsqueeze(1)\n",
    "        return q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c277360c-705b-42b1-8ea2-e9f9aaca4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11714\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 2\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "    - 파란색 / 함수, 주황색 / 데이터\n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # GRU 생성\n",
    "        self.gru = nn.GRU(\n",
    "              input_size=embedding_dim,\n",
    "              hidden_size=hidden_size, \n",
    "              num_layers=num_layers\n",
    "              )\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        질문의 token한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        \n",
    "        Args:\n",
    "            x: 한개 토큰. shape-[1]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size]\n",
    "        Returns\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        # x shape : [batch : 1]\n",
    "        embedded = self.embedding(x).unsqueeze(0) # (batch : 1, embedding_dim) -> (batch : 1, seq_len : 1, embedding_dim)\n",
    "        # 원래는 batch와 seq_len 바꿔야 하지만 한 문장의 토큰 한개씩 입력되서 batch, seq_len 둘다 1이므로 안바꿔도 됌\n",
    "        out, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    # shape 맞춰주기 위한 함수\n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 timestep에서 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312985",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attention Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p, max_length):\n",
    "        # num_vocabs : 총 어휘수\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # attention weight를 계산하는 Linear\n",
    "        # 이전 단어의 hidden state(prev_hidden), 현재 단어의 embedding_vector에 가중합을 계산해서 attention weight를 계산.\n",
    "        # input : 가중합 계산을 위한 hidden_size와 embedding_dim의 합\n",
    "        # output : 각 token에 가중치를 계산해 주기위한 연산이므로 token의 개수인 max_length\n",
    "        self.attn= nn.Linear(hidden_size + embedding_dim, max_length)\n",
    "\n",
    "        # attention value : attn @ 각 토큰의 hidden_state : 내적해서 가중합 계산\n",
    "        # 현재 단어 embedding vector + attention value를 입력받아 가중합을 계산.\n",
    "        # GRU(RNN)에 입력할 입력값을 출력.\n",
    "        self.attn_combine = nn.Linear(hidden_size + embedding_dim, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # 분류기\n",
    "        self.classifier = nn.Linear(hidden_size, num_vocabs)\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰(단어) id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 단어별 다음 단어일 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x).unsqueeze(0) # [batch : 1, seq_len : 1] : shape 맞춰주기\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        # attention weight 계산.\n",
    "        # 입력 : embedding vector + prev_hidden (concat)\n",
    "        # pytorch -> tensor 합칠 때 사용하는 함수 torch.concat([합칠 대상, ..], dim = 방향축)\n",
    "        attn_in = torch.concat((embedding[0], hidden[0]), dim = 1)\n",
    "        # attn_in shape : [1: batch, embedding_dim + hidden_size]\n",
    "        attn_score = self.attn(attn_in) # out : logit(softmax 적용 전) / shape [1, max_length]\n",
    "        attn_weight =nn.Softmax(dim = -1)(attn_score)\n",
    "        \n",
    "        # torch.bmm() - batch-wise matrix multiplication(배치단위 행렬곱)\n",
    "        # 3차원 배열을 받아서 1, 2축 기준으로 행렬 곱 계산.\n",
    "        # (5, 2, 3) @ (5, 3, 5) = 2 * 3 @ 3 * 5 5개를 행렬곱 -> (5, 2, 5)\n",
    "        # attention value 계산 (attn_aplied) - attn_weight(1, max-length) @ encoder_hiddenstate (1, max_length, hidden_size)\n",
    "        attn_value = torch.bmm(\n",
    "            attn_weight.unsqueeze(0),\n",
    "            encoder_outputs.unsqueeze(0),\n",
    "        ) # 결과 (1 : batch, 1 : seq_len, hidden_size)\n",
    "\n",
    "        # attn_combine : GRU에 input값을 생성.\n",
    "        # attn_value + embedding_vactor (concat) -> linear -> relu(비선형성을 위한 함수)\n",
    "        attn_combine_in = torch.concat(\n",
    "            [attn_value[0], embedding[0]], dim = 1\n",
    "            )\n",
    "        gru_in = self.attn_combine(attn_combine_in) # 출력 (batch : 1, hidden_size)\n",
    "        gru_in = gru_in.unsqueeze(0)\n",
    "        gru_in = nn.ReLU()(gru_in)\n",
    "\n",
    "        # gru에 입력해서 다음 단어를 찾기위한 hidden state 계산.\n",
    "        out, hidden_state = self.gru(gru_in, hidden)\n",
    "\n",
    "        # classifier 에 out을 입력해서 다음 단어를 예측\n",
    "        last_out = self.classifier(out[0])\n",
    "        # last_out.shape : [batch : 1, num_vocabs]\n",
    "\n",
    "        return last_out[0], hidden_state, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b8f0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder / Decoder dummy data로 확인.\n",
    "\n",
    "dummy_encoder = Encoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    num_layers= 1\n",
    ")\n",
    "\n",
    "dummy_encoder = dummy_encoder.to(device)\n",
    "\n",
    "dummy_decoder = AttentionDecoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=1,\n",
    "    dropout_p=0.3,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "dummy_decoder = dummy_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b1ecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# 첫번째 질문의 첫번쨰 토큰을 입력 x[0]\n",
    "# hidden state (이전 처리 결과가 없으므로 0)\n",
    "encoder_out, encoder_hidden = dummy_encoder(x[0], dummy_encoder.init_hidden(device))\n",
    "encoder_out.shape, encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13020e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 답변의 첫번째 토큰을 입력 y[0]\n",
    "encoder_outputs = torch.randn(20, 256, device=device) # seq_len : 20, hidden_size : 256\n",
    "next_token, hidden_state, attn_weight = dummy_decoder(y[0], encoder_out, encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a81b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7044])\n",
      "tensor(3405) 헤어져\n",
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(next_token.shape)\n",
    "print(next_token.argmax(-1), tokenizer.id_to_token(next_token.argmax(-1).item()))\n",
    "print(hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cc142f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0468, 0.0603, 0.0492, 0.0535, 0.0515, 0.0547, 0.0475, 0.0546, 0.0504,\n",
       "         0.0421, 0.0555, 0.0551, 0.0484, 0.0458, 0.0500, 0.0518, 0.0462, 0.0453,\n",
       "         0.0413, 0.0499]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(attn_weight.shape)\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184448-56e5-4c3c-8d6c-3f8f93d45076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer 쌍을 학습\n",
    "def train(\n",
    "        input_tensor, # 질문 1개\n",
    "        target_tensor, # 답변 1개\n",
    "        encoder, # Encoder\n",
    "        decoder, # Attention Decoder\n",
    "        encoder_optimizer, # encoder optimizer\n",
    "        decoder_optimizer, # decoder optimizer\n",
    "        loss_fn, # loss 함수\n",
    "        device,\n",
    "        max_length,\n",
    "        teacher_forcing_ratio=0.9):\n",
    "    \n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    loss = 0.0\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Encoder 처리\n",
    "    encoder_hidden = encoder.init_hidden(device) # 첫번째 timestep에 입력할 hidden state\n",
    "\n",
    "    # 질문 / 답변의 length(토큰수)를 조회\n",
    "    input_length = input_tensor.shape[0]\n",
    "    output_length = target_tensor.shape[0]\n",
    "    \n",
    "    # encoder hidden state들을 저장할 tensor를 정의.\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "\n",
    "    # 질문 문장의 token별 hiddne state를 계산 -> encoder_outputs에 저장.\n",
    "    for e_idx in range(input_length):\n",
    "        encoder_out, encoder_hidden = encoder(input_tensor[e_idx], encoder_hidden)\n",
    "        encoder_outputs[e_idx] = encoder_out\n",
    "\n",
    "    # Decoder 처리(답변생성)\n",
    "    # 첫번째 timestep의 토큰 : [SOS]\n",
    "    decoder_input = torch.tensor([SOS_TOKEN], device=device) # decode_input : 현재 timestep의 input\n",
    "    decoder_hidden = encoder_hidden # 첫번째 hidden : context_vector (encoder의 마지막 hidden state)\n",
    "\n",
    "    #teacher_forcing 여부\n",
    "    teacher_forcing = True if teacher_forcing_ratio > random.random() else False\n",
    "\n",
    "    # Decoder 작업 -> (다음 단어 예측(생성)\n",
    "    for d_idx in range(output_length):\n",
    "        # decoder_out : 다음단어 예측값, decoder_hidden : GRU의 hidden state\n",
    "        decoder_out, decoder_hidden, decoder_weight = decoder(\n",
    "            decoder_input, # 1 : [SOS], 2 이후 : decoder의 예측값 or target에서나온 정답\n",
    "            decoder_hidden, #  1 : encoder의 마지막 hidden state / 2 이후 :전 timestep에서 나온 hidden\n",
    "            encoder_outputs \n",
    "        )\n",
    "        # loss 계산\n",
    "        loss += loss_fn(decoder_out.unsqueeze(0), target_tensor[d_idx])\n",
    "\n",
    "        # 다음 timestep의 넣을 input token을 생성 -> decoder_input\n",
    "        if teacher_forcing:\n",
    "            decoder_input = target_tensor[d_idx]\n",
    "        else:\n",
    "            output_token = decoder_out.argmax(dim = -1).unsqueeze(0)\n",
    "            decoder_input = output_token.detach() \n",
    "            # tensor.detach() : gradient 계산그래프에서 제외(역전파 계산에서 제외) \n",
    "            # -> output_token은 decoder_input에 넣어주기 위한 용도의 변수이기 때문에.\n",
    "        \n",
    "        teacher_forcing_ratio *= 0.99\n",
    "\n",
    "        if decoder_input == EOS_TOKEN: # 생성한 단어가 [EOS]이면 종료\n",
    "            break \n",
    "\n",
    "    # 순전파가 완료 (질문 -> 답변) -> 역전파 gradient 계산 / 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / output_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(\n",
    "        encoder, decoder, n_iters, \n",
    "        dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "        # n_iters : 학습시킬 데이터(Q-A쌍)의 개수\n",
    "        # log_interval : train loss를 몇개의 데이터 학습마다 출력할지. / 간격\n",
    "        \n",
    "        # Encoder / Decoder 모델을 train 모드로 변환\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        print_loss = 0.0 # 출력할 loss 값 (출력하면 0으로 초기화)\n",
    "\n",
    "        # 옵티마이저 생성\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "        # loss함수 정의\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 학습 시킬 데이터를 sampling\n",
    "        data_length = len(dataset)\n",
    "        train_data = [dataset[random.randint(0,data_length-1)] for i in range(n_iters)]\n",
    "\n",
    "        # 학습 - train\n",
    "        s = time.time()\n",
    "        for idx in range(n_iters):\n",
    "                input_tensor, target_tensor = train_data[idx]\n",
    "                loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "                             decoder_optimizer, loss_fn, device, max_length=MAX_LENGTH)\n",
    "                print_loss += loss\n",
    "                if (idx+1) % log_interval == 0:\n",
    "                        print(f\"{idx+1}개 QA쌍 학습 : loss - {print_loss/log_interval:.5f}\")\n",
    "                        print_loss = 0\n",
    "\n",
    "        e = time.time()\n",
    "\n",
    "\n",
    "        print(\"걸린시간 : \",e-s)\n",
    "\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cd1f5-0f7f-4271-9143-87b978d8e376",
   "metadata": {},
   "source": [
    "## Model 생성, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f34a5a9-a869-4764-ad59-917c3cc027cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파리터들 정의\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "HIDDEN_SIZE = 200\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.2\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "# 인코더 / 디코더\n",
    "encoder = Encoder(VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, num_layers=1)\n",
    "\n",
    "decoder = AttentionDecoder(VOCAB_SIZE, hidden_size=HIDDEN_SIZE, embedding_dim=EMBEDDING_DIM,\n",
    "                           dropout_p=DROPOUT_P, max_length=MAX_LENGTH)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f175419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500개 QA쌍 학습 : loss - 6.05435\n",
      "1000개 QA쌍 학습 : loss - 5.73009\n",
      "걸린시간 :  122.56637454032898\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "log_interval = 500\n",
    "\n",
    "train_iterations(encoder, decoder, n_iters=n_iters, dataset=dataset, device=device, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b436b71-3cf4-45e2-9418-00555e55d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 인코더, 디코드\n",
    "root_path = \"saved_models/chatbot_attn\"\n",
    "os.makedirs(root_path,  exist_ok=True)\n",
    "\n",
    "tokenizer_path = os.path.join(root_path, \"tokenizer.json\")\n",
    "encoder_path = os.path.join(root_path, \"encoder_model.pt\")\n",
    "decoder_path = os.path.join(root_path, \"decoder_model.pt\")\n",
    "\n",
    "# tokenizer.save(tokenizer_path)\n",
    "# torch.save(encoder, encoder_path)\n",
    "# torch.save(decoder, decoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e95558-b4ed-4676-a23a-796cf0cfa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"saved_models/chatbot_attn\"\n",
    "os.makedirs(root_path,  exist_ok=True)\n",
    "\n",
    "tokenizer_path = os.path.join(root_path, \"tokenizer.json\")\n",
    "encoder_path = os.path.join(root_path, \"encoder_model.pt\")\n",
    "decoder_path = os.path.join(root_path, \"decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8038aca1-7a9e-464a-8558-957a498248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 Load\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "encoder = torch.load(encoder_path, weights_only=False, map_location=device) # 학습시킨 환경이 다를경우 cpu/gpu -> map_location = 현재 device의 상태\n",
    "decoder = torch.load(decoder_path, weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b25ff9-0854-4583-b4f2-0dbc84e63e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = tokenizer.token_to_id('[SOS]')\n",
    "EOS_TOKEN = tokenizer.token_to_id('[EOS]')\n",
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.shape[0]  # 질문 문장 토큰 개수.\n",
    "        encoder_hidden = encoder.init_hidden(device) # 첫 timestep에 넣어줄 hidden state\n",
    "\n",
    "        # encoder의 hidden state들을 모을 텐서 생성\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        # encoder 실행\n",
    "        for e_index in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[e_index], encoder_hidden)\n",
    "            encoder_outputs[e_index] = encoder_output[0, 0]\n",
    "\n",
    "        # decoder 실행\n",
    "        decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # 결과를 저장할 리스트\n",
    "        decoded_words = []  # 디코더가 추론한 단어(토큰)들을 저장.\n",
    "        decoder_attn_weights = [] # 각 단어들을 추론할 때 계산된 attention weight값들을 저장.\n",
    "\n",
    "        for d_index in range(max_length):\n",
    "            decoder_output, decoder_hidden, attn_weight = decoder(decoder_input, \n",
    "                                                                  decoder_hidden, \n",
    "                                                                  encoder_outputs)\n",
    "            decoder_attn_weights.append(attn_weight.data)\n",
    "\n",
    "            topv, topi  = decoder_output.data.topk(1)\n",
    "\n",
    "            if topi.item() == EOS_TOKEN:\n",
    "                decoded_words.append('[EOS]')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.tokenizer.id_to_token(topi.item()))\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "    return decoded_words, decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e33d28a-5414-4d8e-8919-730109adfd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_tokens(decoded_string):\n",
    "    \"\"\"\n",
    "    Subword 처리\n",
    "    subword는 단어의 시작으로 쓰인 것과 중간 부분(연결)에 사용된 두가지 subword가 있다.  연결 subword는 `#`과 같은 특수문자로 시작 한다.\n",
    "    tokenizer.decode() 결과 문자열은 subword의 특수문자('##')을 처리하지 않는다. 이것을 처리하는 함수\n",
    "    ex) \"이 기회 ##는 내 ##꺼 #야\" ==> \"이 기회는 내꺼야\"\n",
    "    \n",
    "    Parameter\n",
    "        decoded_string: str - Tokenizer가 decode한 중간 subword의 특수문자 처리가 안된 문자열. \n",
    "    Return\n",
    "        str: subword 특수문자 처리한 문자열\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = decoded_string.split()\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"##\"):\n",
    "            if new_tokens: # len(new_tokens) != 0 원소가 하나라도 있으면\n",
    "                # 토큰에서 ##을 제거하고 리스트의 마지막 원소(문자열) 뒤에 붙인다.\n",
    "                new_tokens[-1] += token[2:]\n",
    "            else: # new_tokens가 빈 리스트. 현재 token이 첫번째 단어. ##을 지우고 append\n",
    "                new_tokens.append(token[2:])\n",
    "        else: # 단어의 시작인 토큰. (##이 없는 토큰) -> list에 추가.\n",
    "            new_tokens.append(token)\n",
    "        \n",
    "    return \" \".join(new_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9451766-9127-47ed-b844-db9c71d1b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(encoder, decoder, dataset, device, n=10):\n",
    "    # n개 확인.\n",
    "    for i in range(n):\n",
    "        idx = random.randint(0, len(dataset))\n",
    "        x, y = dataset[idx]\n",
    "        q = dataset.tokenizer.decode(x.flatten().tolist())\n",
    "        a = dataset.tokenizer.decode(y.flatten().tolist())\n",
    "        print(\"질문(정답):\", handle_special_tokens(q))\n",
    "        print(\"답변(정답):\", handle_special_tokens(a))\n",
    "\n",
    "        # 추론\n",
    "        output_words, atten_weights = evaluate(encoder, decoder,\n",
    "                                              x.to(device), \n",
    "                                              dataset, device, MAX_LENGTH)\n",
    "        # output_words: [단어, 단어, 단어, ....]\n",
    "        output_sentence = ' '.join(output_words[:-1]) # [EOS]는 제거\n",
    "        print(\"답변(예측):\", handle_special_tokens(output_sentence))\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41a6557b-7b69-4e2f-93af-9eb38621b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문(정답): 이별 5달\n",
      "답변(정답): 이젠 마음의 정리가 끝났길 바랍니다 .\n",
      "답변(예측): 아니라고 슴 잘해주구나 거울 변화를 마시끼리 썸이 끝도 끝도 꾸 봄 짝녀에게 어느꿀 치워꿀 치워\n",
      "==================================================\n",
      "질문(정답): 썸남한테 같이 카공하자고 할까 ?\n",
      "답변(정답): 카공 좋죠 !\n",
      "답변(예측): 때론 1년째 잡으세요끼리 잘했어요보는보는끼리 넘어 썸이 끝도 끝도 끝도 꾸나봐 곁 잘해주 치워 넘어\n",
      "==================================================\n",
      "질문(정답): 허전해\n",
      "답변(정답): 채워질 거예요 .\n",
      "답변(예측): 아니라고 슴 짝녀에게 어느꿀 끝은 끝은 회사 치워 치워 윤릴까 잘해주 치워 윤릴까 잘해주 치워 넘어\n",
      "==================================================\n",
      "질문(정답): 잊을 수가 있을까\n",
      "답변(정답): 힘들긴 하지만 시간이 약이라는 사실을 잊지 말아요 .\n",
      "답변(예측): 아니라고 슴 잘해주 깡 치워 치워 윤릴까 잘해주 치워 윤릴까 잘해주 치워 윤릴까 잘해주 치워 윤\n",
      "==================================================\n",
      "질문(정답): 골프 배워야 돼\n",
      "답변(정답): 시간내서 가보세요 .\n",
      "답변(예측): 오래된 연락은은데보는 태어나 잡으세요 치워꿀 치워 있더라고요 윤릴까부 옴 벗어나 잘했어요 회사 치워 치워\n",
      "==================================================\n",
      "질문(정답): 교회에 좋아하는 오빠가 생겼어 .\n",
      "답변(정답): 일요일이 기다려지겠네요 .\n",
      "답변(예측): 잊찾 아니지만 곁 잘해주 벗어나 회사 치워꿀 치워 윤 잘했어요 치워꿀 치워 넘어 썸이 뭐라고 이유를\n",
      "==================================================\n",
      "질문(정답): 과연 사랑이였을까 ?\n",
      "답변(정답): 마음이 알고 있을 거예요 .\n",
      "답변(예측): 깨달 거울 봄 툼 킬 귤 남자친구를스러워꿀 귤스러워 꼈꿀꿀물 책례 국 X\n",
      "==================================================\n",
      "질문(정답): 오늘 야근인가\n",
      "답변(정답): 오늘도 고생이 많으시네요 .\n",
      "답변(예측): 아니라고 슴 짝녀에게 어느꿀 끝은 끝은 회사 치워 치워 윤릴까 잘해주 치워 윤릴까 잘해주 치워 넘어\n",
      "==================================================\n",
      "질문(정답): 짝남 표정 안좋아서 괜히 신경 쓰여 .\n",
      "답변(정답): 신경 쓰일거라 생각해요 .\n",
      "답변(예측): 초대은데 꿰 나를 어느꿀 끝은 끝은 회사 치워 치워 윤릴까 잘해주 치워 넘어 썸이 뭐라고 나를\n",
      "==================================================\n",
      "질문(정답): 내 번호를 따간 사람이 내 친구가 좋아하는 애야 .\n",
      "답변(정답): 운명의 장난같네요 .\n",
      "답변(예측): 아니라고 슴 짝녀에게 어느꿀 끝은 끝은 회사 치워 치워 윤릴까 잘해주 치워 넘어 썸이 뭐라고 이유를 나를\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder, decoder, dataset, device, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e2587-ed54-4b58-b8d3-9c3b1dd4b60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
