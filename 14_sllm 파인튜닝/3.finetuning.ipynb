{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2591659b-83e1-45d7-b166-2c85a2f74367",
   "metadata": {},
   "source": [
    "##  필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f59b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate trl peft langchain python-dotenv -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226ced1",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb0bd5-8cee-41df-8385-758601e8051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv())\n",
    "\n",
    "login(os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef323ce1-aeb4-446d-a331-6468e83d42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_id = \"kgmyh/naver_economy_news_stock_instruct_dataset\"\n",
    "\n",
    "dataset = load_dataset(data_id)\n",
    "\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e70e3c",
   "metadata": {},
   "source": [
    "# sLLM Model Load\n",
    "\n",
    "## Base Model Load\n",
    "- 한국어를 충분히 학습한 모델을 선택한다.\n",
    "\n",
    "- kakao의 kanana 모델을 base 모델로 파인튜닝을 진행한다.\n",
    "  - https://tech.kakao.com/posts/660\n",
    "  - https://github.com/kakao/kanana\n",
    "  - 2.1b 모델을 open source로 공개함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cd2a7-ac0b-4c8c-8926-c1d5d80f2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = \"kakaocorp/kanana-1.5-2.1b-instruct-2505\" # JSON 포멧은 잘 지켜서 나오는데 회사나 이런게 없는 회사가 나오고 한다.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\" \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515577ed",
   "metadata": {},
   "source": [
    "### 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fa57b-c012-4220-9259-b33d760b148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################################\n",
    "#  입력 프롬프트 생성\n",
    "#\n",
    "#  - kanana, Llama 동일한 프롬프트 형식 (자세한 형식은 아래 있다.)\n",
    "#    - Instruction 모델이 학습할 때 사용한 prompt 형식에 맞춰 입력데이터를 변환을 해야 한다.\n",
    "#  - `apply_chat_template`: 입력 형태 기본: {\"role\":\"역할\", \"content\":\"content\"}  이 형식으로 입력을 하면 모델의 입력형식으로 변환해 주는 메소드.\n",
    "####################################################################################################################################################\n",
    "content = \"오늘 서울 날씨 어때요?\"\n",
    "content = \"오늘 서울 날씨 어때요? 모르면 모른다고 답하세요.\"\n",
    "message = [  \n",
    "        {\"role\": \"system\", \"content\": \"당신은 인공지는 날씨 예보관입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize=False,            # 토큰 id로 반환할 지 여부 (여기서는 텍스트 확인을 위해 False)\n",
    "    add_generation_prompt=True # 마지막에 assistant  role에 대한 역할 식별자를 붙일 지 여부. (마지막에 `답변:` 이렇게 넣는 것과 같다.)\n",
    ")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 토큰화\n",
    "########################\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25256f7a-8401-4b24-ba07-e96d753340f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### \n",
    "# 생성\n",
    "###################\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,  # 답변 토큰 수 제한\n",
    "        do_sample=True,      # 확률 붙포에 따라 다음 토큰을 무작위로 선택(True - top-k, temperature와 같이 쓰인다.), False: 가장 높은 확률의 토큰만 선택\n",
    "        top_p=0.95,          # 뉴클리어스(nucleus) 샘플링의 누적 확률 임계값 - 다음에 올 확률 순으로 토큰들 정렬 → 누적 → 0.95(지정한값) 도달 시점까지 상위토큰만 남긴다. ->  집합에서 샘플링. 낮으면(예: 0.5) 창의성↓, 너무 높으면(1.0) 창의성↑\n",
    "        temperature=0.8,     # 토큰의 다양성을 지정. 낮을수록(0에 가까울수록) 높은 확률의 토큰을 더욱 선택. 낮으면(예: 0.5) 창의성↓, 너무 높으면(1.0) 창의성↑\n",
    "        pad_token_id=tokenizer.eos_token_id,  # 패딩 토큰 ID. 대다수 LLM은 별도의 pad_token이 없이 eos_token(end of sequence token)으로 대신함\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2375848",
   "metadata": {},
   "source": [
    "# 학습 전에 추론 시켜 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6f0af",
   "metadata": {},
   "source": [
    "### 추론용 프롬프트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#  System 프롬프트\n",
    "######################################\n",
    "\n",
    "system_prompt = '''# Instruction\n",
    "당신은 금융 뉴스의 핵심 내용을 요약해 설명하고, 뉴스가 특정 상장 종목에 미치는 긍정/부정 영향 여부, 이유, 근거 등을 분석하는 금융 분석 전문가입니다.\n",
    "사용자에 의해 입력된 뉴스 기사를 분석해서 **한국에 상장된 주식 종목에 영향을 주는지 판단**하고, Output Indicator에 제시된 기준에 따라 구조화된 JSON 형식으로 결과를 출력하세요.\n",
    "\n",
    "## 분석 기준\n",
    "1. 뉴스가 **한국 주식 종목에 영향을 주는지 판단**하세요.\n",
    "2. 영향을 준다면 다음 항목을 출력하세요.\n",
    "   - `\"is_stock_related\": true`\n",
    "   - 뉴스에 **긍정적** 영향을 받는 **회사이름들**\n",
    "   - 뉴스에 **부정적** 영향을 받는 **회사이름들**\n",
    "   - 뉴스가 각 회사에 **긍정적 또는 부정적 영향을 주는지 이유**\n",
    "     - 반드시 **뉴스기사에 언급된 내용 기반으로 작성한다.** 뉴스기사에 없는 내용을 꾸며서 임의로로 작성하지 않습니다.\n",
    "     - `None`, 유추, 추정, 일반 논평 금지합니다.\n",
    "   - 뉴스 요약 (3줄 이내)\n",
    "3. 뉴스가 한국 주식 종목에 영향을 주지 않는다면 다음 항목을 출력하세요.\n",
    "   - `\"is_stock_related\": false`\n",
    "   - 뉴스 요약 (3줄 이내)\n",
    "\n",
    "# 출력 지시사항 (Output Indicator)\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "```\n",
    "{\"properties\": {\"is_stock_related\": {\"description\": \"한국 주식과 관련있는 뉴스인지 여부\", \"title\": \"Is Stock Related\", \"type\": \"boolean\"}, \"positive_stocks\": {\"description\": \"뉴스기사에 긍정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Positive Stocks\", \"type\": \"array\"}, \"positive_reason\": {\"description\": \"뉴스내용 중 positive_stocks에 있는 각 회사들에 긍정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"긍정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Positive Reason\", \"type\": \"array\"}, \"negative_stocks\": {\"description\": \"뉴스기사에 부정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Negative Stocks\", \"type\": \"array\"}, \"negative_reason\": {\"description\": \"뉴스내용 중 negative_stocks에 있는 각 회사들에 부정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"부정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Negative Reason\", \"type\": \"array\"}, \"summary\": {\"description\": \"뉴스기사 요약\", \"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"is_stock_related\", \"positive_stocks\", \"positive_reason\", \"negative_stocks\", \"negative_reason\", \"summary\"]}\n",
    "```\n",
    "\n",
    "## 출력 조건:\n",
    "- 뉴스에 영향을 받은 회사들은 **반드시 한국 증시에 상장된 종목** 이어야 합니다.\n",
    "- 뉴스에 있는 내용만 출력결과에 포함시킵니다.\n",
    "- 긍정/부정 종목은 실제 뉴스기사에 영향을 받는 회사들만 포함하세요.\n",
    "- 모든 문자열은 큰따옴표(`\"`)로 감쌉니다.\n",
    "- 문자열 안에 따옴표가 필요하면 작은따옴표(`'`)를 사용합니다.\n",
    "- 모든 키(Key)는 출력 지시사항에 명시된 property들과 정확히 일치해야 합니다.\n",
    "- `\"positive_reasons\"` 및 `\"negative_reasons\"`의 값은 `None`이 될 수 없습니다.\n",
    "- json format을 잘 지켜 응답데이터를 만듭니다. 배열이나 object의 마지막 항목 뒤에 `,` 를 붙이지 마세요.\n",
    "- 오직 유효한 JSON 문자열(UTF-8, RFC8259 준수)만 출력합니다.\n",
    "- 절대 다른 텍스트, 주석, 설명, 코드 블록 표기(```), 또는 따옴표 외의 문자열을 추가하면 안 됩니다.\n",
    "\n",
    "## 출력 예시 (Examples)\n",
    "\n",
    "### 뉴스가 특정 주식종목들에 **긍정적 영향이 주는 경우**:\n",
    "{'is_stock_related': True,\n",
    " 'negative_reasons': [],\n",
    " 'negative_stocks': [],\n",
    " 'positive_reasons': [{'세라젬': '루게릭병 환우 지원 캠페인 후원과 의료가전 지원 등 사회공헌활동을 통해 기업 이미지와 브랜드 가치가 긍정적으로 부각됨'}],\n",
    " 'positive_stocks': ['세라젬'],\n",
    " 'summary': '세라젬이 루게릭병 환우를 위한 아이스버킷 챌린지 런 행사를 후원하며 의료가전과 건강기능식품 등을 지원했다. 캠페인은 루게릭병 환우 지원과 기부 문화 확산을 목표로 한다. 세라젬은 다양한 사회공헌활동을 지속하고 있다.'\n",
    "}\n",
    "\n",
    "### 뉴스의 내용이 특정 주식종목들에 **부정적 영향이 주는 경우**:\n",
    "{\n",
    "    \"is_stock_related\": true,\n",
    "    \"positive_stocks\": [],\n",
    "    \"positive_reasons\": [],\n",
    "    \"negative_stocks\": [\n",
    "        \"포스코\",\n",
    "        \"현대제철\"\n",
    "    ],\n",
    "    \"negative_reasons\": [\n",
    "        {\"포스코\": \"정부가 수입규제국 조사에 적극 대응하고 비관세장벽 해소를 위해 민관 협력 강화 방침을 밝혀 철강 분야에서 수출 피해 최소화 기대\"},\n",
    "        {\"현대제철\": \"철강·금속 품목에 대한 수입규제 대응 강화로 불합리한 무역제한 조치 개선 가능성이 높아져 수출 환경 개선 기대\"}\n",
    "    ],\n",
    "    \"summary\": \"산업부는 수입 규제국의 조사에 대응하고 비관세장벽 해소를 위한 협의를 진행했다. 규제 대상 국가는 26개국, 건수는 199건에 달한다.\"\n",
    "}\n",
    "\n",
    "### **뉴스기사가 주식 종목과 관련 없는 경우**:\n",
    "{\n",
    "    \"is_stock_related\": false,\n",
    "    \"positive_stocks\": [],\n",
    "    \"positive_reasons\": [],\n",
    "    \"negative_stocks\": [],\n",
    "    \"negative_reasons\": [],\n",
    "    \"summary\": \"정황근 농림축산식품부 장관이 단순가공식품 부가가치세 면제 시행 상황을 점검했다. 된장, 고추장 코너를 방문하며 현장을 살폈다.\"\n",
    "}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946cdab-71fb-458a-a7f8-fd6f64104059",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 추론할 1개의 샘플 데이터 생성\n",
    "# 뉴스 기사 제목 + \"\\n\\n\" + 뉴스 기사 내용\n",
    "################################################################\n",
    "\n",
    "idx = 10\n",
    "\n",
    "sample = dataset['train'][idx]\n",
    "user_input = sample['document']+\"\\n\\n\"+sample['document']\n",
    "prompt = [\n",
    "    {\"role\":\"system\", \"content\":system_prompt},\n",
    "    {\"role\":\"user\", \"content\": user_input}\n",
    "]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d9515-d939-456f-8da4-271ca3f83caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Pipeline을 이용해 실행\n",
    "#  - task: text-generation\n",
    "#  - pipeline은 (role base) chat format 으로 입력한다. \n",
    "#########################################################\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer )\n",
    "res = pipe(prompt,  max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3343d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca101393",
   "metadata": {},
   "source": [
    "# 파인 튜닝 \n",
    "\n",
    "## 데이터셋 만들기\n",
    "\n",
    "### 프롬프트 생성\n",
    "- LLM 모델은 학습할때 사용한 프롬프트 형식이 있다.\n",
    "    - 모델 마다 형식이 다르기 때문에 확인이 필요하다.\n",
    "    - 모델이 사용한 tokenizer의 chat_template 속성을 이용해 조회할 수 있다.\n",
    "      - `tokenizer.chat_template`\n",
    "      - `jija2` 템플릿 엔진 문법으로 작성됨.\n",
    "- 학습데이터를 LLM의 프롬프트 형식으로 생성한다.\n",
    "\n",
    "### Llama 모델의 chat template 형식\n",
    "```\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "[시스템 역할 지침]<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "[유저 질문]<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "[모델의 답변]<|eot_id|>\n",
    "```\n",
    "- `<|begin_of_text|>`: 시퀀스의 시작을 나타내는 토큰.\n",
    "- `<|start_header_id|>ROLE<|end_header_id|>`: Role(발화자) 지정 - system | user | assistant\n",
    "- `메세지 내용<|eot_id|>`:  Role 메세지. 시퀀스 종료를 나타내는 토큰(`<|eot_id|>`)\n",
    "\n",
    "#### 예\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 인공지는 날씨 예보관입니다.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "오늘 서울 날씨 어때요?<|eot_id|>\n",
    "```\n",
    "> ### Gemma format chat template 형식\n",
    "> \n",
    "> ```xml\n",
    "> <bos><start_of_turn>Role\n",
    "> {사용자 입력-input}<end_of_turn>\n",
    "> <start_of_turn>model\n",
    "> {AI 답변-label}<end_of_turn>\n",
    "> ```\n",
    "> -\t`<bos>`: 시퀀스의 시작을 나타내는 토큰.\n",
    "> -\t`<start_of_turn>`: 각 대화의 시작을 나타낸다.\n",
    "> -\t`Role`: Role(발화자) 지정 - **user** | **model** \n",
    ">   - gemma는 user와 model 두가지 role을 이용해 instruction 모델을 학습함.\n",
    ">   - gemma 는 system role을 사용하지 않는다. 그래서 **system prompt는 user role의 content에 넣어준다.**\n",
    "> - `메세지`\n",
    ">   - Role 다음 줄에 이어서 메세지를 입력한다.\n",
    "> - `<end_of_turn>`: 대화의 끝을 나타냅니다.\n",
    "> - 예\n",
    "> ```xml\n",
    "> <bos><start_of_turn>user\n",
    "> AI에 대해 설명해주세요.<end_of_turn>\n",
    "> <start_of_turn>model\n",
    "> AI은 컴퓨터 시스템이 인간의 지능적 기능을 모방하여 데이터를 처리하고 의사결정을 수행하는 기술입니다.<end_of_turn>\n",
    "> ```\n",
    "\n",
    "> ### Alapaca format chat template\n",
    "> ```\n",
    "> ### Instruction:\n",
    "> [시스템 역할 지침]\n",
    "> \n",
    "> [유저 질문]\n",
    "> \n",
    "> ### Response:\n",
    "> [모델의 답변]\n",
    "> ```\n",
    "> - `### Instruction`: - 사용자 입력/질문\n",
    "> - `### Response`: - 모델의 답변\n",
    "> - 특수 토큰 대신 마크다운 스타일의 헤더 사용\n",
    "> - 보통 맨 앞에 표준 instruction 문구 포함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16af714-fc82-404b-a072-300b9a394da3",
   "metadata": {},
   "source": [
    "### InputPromptCreator(프롬프트 생성 클래스) 정의\n",
    "\n",
    "- `create_pipeline_prompt()`:\n",
    "    - 뉴스기사 제목, 뉴스기사내용을 받아서 파이프라인에 입력할 chat 형식 프롬프트생성한다\n",
    "- `create_generate_prompt:()`:\n",
    "    - 뉴스기사 제목, 뉴스기사내용을 받아서  모델의 자체 chat 형식의 프롬프트를 생성한다.\n",
    "- `create_train_prompt()`:\n",
    "    - Dataset의 개별 데이터를 입력받아서 모델 학습을 위한 chat 프롬프트생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610af68-8ee5-44eb-b372-fceac8eb935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# 학습 용 프롬프트 생성 함수\n",
    "####################################################################\n",
    "\n",
    "\n",
    "from textwrap import dedent\n",
    "class InputPromptCreator:\n",
    "    \"\"\"모델별 형식에 맞춰 입력 프롬프트를 생성한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer=None):\n",
    "        \"\"\"\n",
    "        모델의 chat template을 제공하는 tokenizer와 system 프롬프트를 받아서 초기화\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = dedent('''\n",
    "        # Instruction\n",
    "        당신은 금융 뉴스의 핵심 내용을 요약해 설명하고, 뉴스가 특정 상장 종목에 미치는 긍정/부정 영향 여부, 이유, 근거 등을 분석하는 금융 분석 전문가입니다.\n",
    "        사용자에 의해 입력된 뉴스 기사를 분석해서 **한국에 상장된 주식 종목에 영향을 주는지 판단**하고, Output Indicator에 제시된 기준에 따라 구조화된 JSON 형식으로 결과를 출력하세요.\n",
    "        \n",
    "        ## 분석 기준\n",
    "        1. 뉴스가 **한국 주식 종목에 영향을 주는지 판단**하세요.\n",
    "        2. 영향을 준다면 다음 항목을 출력하세요.\n",
    "           - `\"is_stock_related\": true`\n",
    "           - 뉴스에 **긍정적** 영향을 받는 **회사이름들**\n",
    "           - 뉴스에 **부정적** 영향을 받는 **회사이름들**\n",
    "           - 뉴스가 각 회사에 **긍정적 또는 부정적 영향을 주는지 이유**\n",
    "             - 반드시 **뉴스기사에 언급된 내용 기반으로 작성한다.** 뉴스기사에 없는 내용을 꾸며서 임의로로 작성하지 않습니다.\n",
    "             - `None`, 유추, 추정, 일반 논평 금지합니다.\n",
    "           - 뉴스 요약 (3줄 이내)\n",
    "        3. 뉴스가 한국 주식 종목에 영향을 주지 않는다면 다음 항목을 출력하세요.\n",
    "           - `\"is_stock_related\": false`\n",
    "           - 뉴스 요약 (3줄 이내)\n",
    "        \n",
    "        # 출력 지시사항 (Output Indicator)\n",
    "        The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "        \n",
    "        As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "        the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "        \n",
    "        Here is the output schema:\n",
    "        ```\n",
    "        {\"properties\": {\"is_stock_related\": {\"description\": \"한국 주식과 관련있는 뉴스인지 여부\", \"title\": \"Is Stock Related\", \"type\": \"boolean\"}, \"positive_stocks\": {\"description\": \"뉴스기사에 긍정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Positive Stocks\", \"type\": \"array\"}, \"positive_reason\": {\"description\": \"뉴스내용 중 positive_stocks에 있는 각 회사들에 긍정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"긍정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Positive Reason\", \"type\": \"array\"}, \"negative_stocks\": {\"description\": \"뉴스기사에 부정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Negative Stocks\", \"type\": \"array\"}, \"negative_reason\": {\"description\": \"뉴스내용 중 negative_stocks에 있는 각 회사들에 부정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"부정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Negative Reason\", \"type\": \"array\"}, \"summary\": {\"description\": \"뉴스기사 요약\", \"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"is_stock_related\", \"positive_stocks\", \"positive_reason\", \"negative_stocks\", \"negative_reason\", \"summary\"]}\n",
    "        ```\n",
    "        \n",
    "        ## 출력 조건:\n",
    "        - 뉴스에 영향을 받은 회사들은 **반드시 한국 증시에 상장된 종목** 이어야 합니다.\n",
    "        - 뉴스에 있는 내용만 출력결과에 포함시킵니다.\n",
    "        - 긍정/부정 종목은 실제 뉴스기사에 영향을 받는 회사들만 포함하세요.\n",
    "        - 모든 문자열은 큰따옴표(`\"`)로 감쌉니다.\n",
    "        - 문자열 안에 따옴표가 필요하면 작은따옴표(`'`)를 사용합니다.\n",
    "        - 모든 키(Key)는 출력 지시사항에 명시된 property들과 정확히 일치해야 합니다.\n",
    "        - `\"positive_reasons\"` 및 `\"negative_reasons\"`의 값은 `None`이 될 수 없습니다.\n",
    "        - json format을 잘 지켜 응답데이터를 만듭니다. 배열이나 object의 마지막 항목 뒤에 `,` 를 붙이지 마세요.\n",
    "        - 오직 유효한 JSON 문자열(UTF-8, RFC8259 준수)만 출력합니다.\n",
    "        - 절대 다른 텍스트, 주석, 설명, 코드 블록 표기(```), 또는 따옴표 외의 문자열을 추가하면 안 됩니다.\n",
    "        \n",
    "        ## 출력 예시 (Examples)\n",
    "        \n",
    "        ### 뉴스가 특정 주식종목들에 **긍정적 영향이 주는 경우**:\n",
    "        {'is_stock_related': True,\n",
    "         'negative_reasons': [],\n",
    "         'negative_stocks': [],\n",
    "         'positive_reasons': [{'세라젬': '루게릭병 환우 지원 캠페인 후원과 의료가전 지원 등 사회공헌활동을 통해 기업 이미지와 브랜드 가치가 긍정적으로 부각됨'}],\n",
    "         'positive_stocks': ['세라젬'],\n",
    "         'summary': '세라젬이 루게릭병 환우를 위한 아이스버킷 챌린지 런 행사를 후원하며 의료가전과 건강기능식품 등을 지원했다. 캠페인은 루게릭병 환우 지원과 기부 문화 확산을 목표로 한다. 세라젬은 다양한 사회공헌활동을 지속하고 있다.'\n",
    "        }\n",
    "        \n",
    "        ### 뉴스의 내용이 특정 주식종목들에 **부정적 영향이 주는 경우**:\n",
    "        {\n",
    "            \"is_stock_related\": true,\n",
    "            \"positive_stocks\": [],\n",
    "            \"positive_reasons\": [],\n",
    "            \"negative_stocks\": [\n",
    "                \"포스코\",\n",
    "                \"현대제철\"\n",
    "            ],\n",
    "            \"negative_reasons\": [\n",
    "                {\"포스코\": \"정부가 수입규제국 조사에 적극 대응하고 비관세장벽 해소를 위해 민관 협력 강화 방침을 밝혀 철강 분야에서 수출 피해 최소화 기대\"},\n",
    "                {\"현대제철\": \"철강·금속 품목에 대한 수입규제 대응 강화로 불합리한 무역제한 조치 개선 가능성이 높아져 수출 환경 개선 기대\"}\n",
    "            ],\n",
    "            \"summary\": \"산업부는 수입 규제국의 조사에 대응하고 비관세장벽 해소를 위한 협의를 진행했다. 규제 대상 국가는 26개국, 건수는 199건에 달한다.\"\n",
    "        }\n",
    "        \n",
    "        ### **뉴스기사가 주식 종목과 관련 없는 경우**:\n",
    "        {\n",
    "            \"is_stock_related\": false,\n",
    "            \"positive_stocks\": [],\n",
    "            \"positive_reasons\": [],\n",
    "            \"negative_stocks\": [],\n",
    "            \"negative_reasons\": [],\n",
    "            \"summary\": \"정황근 농림축산식품부 장관이 단순가공식품 부가가치세 면제 시행 상황을 점검했다. 된장, 고추장 코너를 방문하며 현장을 살폈다.\"\n",
    "        }''')\n",
    "\n",
    "    def create_pipeline_prompt(self, news_title:str, news_document:str) -> list[dict]:\n",
    "        \"\"\"파이프라인에 입력할 chat 형식 프롬프트생성한다.\n",
    "        ```\n",
    "        [\n",
    "            {\"role\":\"system\", \"content\":시스템프롬프트},\n",
    "            {\"role\":\"user\", \"content\": news_title+\"\\n\\n\"+news_document}\n",
    "        ]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        news = news_title+\"\\n\\n\"+news_document\n",
    "        message = [\n",
    "            {\"role\":\"system\", \"content\":self.system_prompt},\n",
    "            {\"role\":\"user\", \"content\": news}\n",
    "        ]\n",
    "        return message\n",
    "\n",
    "    def create_generate_prompt(self, news_title:str, news_document:str) -> str:\n",
    "        \"\"\"뉴스 제목과 내용을 받아서 tokenizer를 이용해 모델의 자체 chat 형식의 프롬프트를 생성한다.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise Exception(\"Tokenizer가 없습니다. generate_prompt를 사용하려면 모델의 tokenizer가 필요합니다.\")\n",
    "        message = self.create_pipeline_prompt(news_title, news_document)\n",
    "        prompt =  tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def create_train_prompt(self, datapoint:dict) -> dict:\n",
    "        \"\"\"\n",
    "        Dataset의 개별 데이터를 입력받아서 모델 학습을 위한 chat 프롬프트생성\n",
    "        프롬프트 형식\n",
    "        ```\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {시스템 프롬프트}<|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {입력 - title+document}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        {답변 - Label}<|eot_id|>\n",
    "        ```\n",
    "        Args:\n",
    "            datapoint (dict): 변환할 데이터\n",
    "    \n",
    "        Returns:\n",
    "            str: 모델 학습을 위한 프롬프트. \n",
    "        \"\"\"\n",
    "        chat_template = dedent('''\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>system<|end_header_id|>\n",
    "            {system_prompt}<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            {input_content}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            {label}<|eot_id|>''')\n",
    "    \n",
    "        content = datapoint['title']+\"\\n\"+datapoint['document']\n",
    "        prompt = chat_template.format(system_prompt=self.system_prompt, input_content=content, label=datapoint['label'])\n",
    "\n",
    "        return {\"train_prompt\":prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb886b27-8d58-4110-90d6-48b1cc8f123b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c9e81-470b-4738-a1a7-172d0d28b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5addeaa7",
   "metadata": {},
   "source": [
    "### InputPromptCreator.create_train_prompt()를 이용해 입력 프롬프트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Trainset\n",
    "##########################\n",
    "\n",
    "llama_input_creator = InputPromptCreator(tokenizer)\n",
    "trainset = train_set.map(llama_input_creator.create_train_prompt, remove_columns=list(train_set.features))\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba79668-30a9-4651-8430-f61ed3047898",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Testset\n",
    "##########################\n",
    "\n",
    "testset = test_set.map(llama_input_creator.create_train_prompt, remove_columns=list(test_set.features))\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801263cf-ac85-4e26-9fc6-dbd057552905",
   "metadata": {},
   "source": [
    "## 파인튜닝\n",
    "\n",
    "### Data Collator\n",
    "\n",
    "- 학습 도중 입력 데이터를 받아 전처리하는 함수\n",
    "  - Dataset -> Data Collator함수 -> 모델\n",
    "    - 데이터셋에서 모델에 전달되는 batch를 받아서 모델에 입력전에 해야하는 처리를 담당하는 함수(Callable).\n",
    "\n",
    "- 구현할 내용\n",
    "  - 모델 chat 형식의 문자열을 transformers 모델에 입력하기 위한 inputs를 만든다.\n",
    "    \n",
    "  ```json\n",
    "  {\n",
    "  \"input_ids\":입력 sequence의 토큰 ID들,\n",
    "  \"attention_mask\":입력토큰과 padding구분,\n",
    "  \"labels\": input_ids에서 답변부분 masking. 답변은 토큰ID 나머지는 -100으로 채운다. \n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_seq_length = 8192\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list[dict]\n",
    "    #  dict - {\"train_prompt\":input text}\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for prompt in batch:\n",
    "        \n",
    "        text = prompt['train_prompt'].strip()\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length, # max_length이하는 자른다. truncation=True\n",
    "            padding=False,             # padding은 뒤에서 수동으로 처리할 것이기 때문에 padding처리하지 않는다.\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids) #모델 응답 부분만 label로 지정: input_ids에서 \"system/user 프롬프트는 -100\", \"label은 실제 토큰값\" 으로 masking한다. 그것을 위해서 input_ids와 동일한 크기의 -100으로 채워진 리스트 생성한다.\n",
    "                                         # -100인 이유: PyTorch의 CrossEntropyLoss(ignore_index=-100)가 loss계산시 무시하는 값이다.\n",
    "\n",
    "        ########################################################\n",
    "        # chat prompt에서 답변 부분을 찾아서 labels를 구성한다. \n",
    "        ########################################################\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)  # assistant_header 토큰 index로 변환\n",
    "\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]\n",
    "                break\n",
    "            i += 1\n",
    "        \n",
    "        ##################################################################\n",
    "        # 생성된 input_ids, attention_mask, labels를 new_batch에 추가한다.\n",
    "        ##################################################################\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "\n",
    "    ######################################################\n",
    "    #  패딩 처리 \n",
    "    #  -  배치내 입력중 가장 긴 sample에 길이를 맞춘다.\n",
    "    ######################################################\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])            \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i]) \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c284a5e-ca60-4df7-93d1-1539f407e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 확인\n",
    "######################\n",
    "example = [trainset[1000], trainset[12], trainset[10]]\n",
    "batch = collate_fn(example)\n",
    "\n",
    "print(\"batch:\")\n",
    "print(\"input_ids 크기:\", batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask 크기:\", batch[\"attention_mask\"].shape)\n",
    "print(\"labels 크기:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da2145",
   "metadata": {},
   "source": [
    "## SFTConfig 설정\n",
    "\n",
    "### LoRA 설정\n",
    "\n",
    "- LoRAConfig 주요 매개변수\n",
    "\n",
    "| 매개변수                | 의미/역할                                         | 주요 옵션·예시                                                                                                             |\n",
    "| ------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n",
    "| **lora\\_alpha**     | LoRA 어댑터의 학습 스케일 팩터(변화 강도). 값이 크면 학습 변화가 완만해짐 | 16, 32, 64 등                                                                                                         |\n",
    "| **lora\\_dropout**   | 어댑터에만 적용되는 드롭아웃 확률. 과적합 방지                    | 0.05, 0.1 등                                                                                                          |\n",
    "| **r**               | LoRA 어댑터의 랭크(정보량/두께). 값이 크면 더 많은 정보, 메모리 사용↑  | 8, 16, 32 등                                                                                                          |\n",
    "| **bias**            | 기존 모델의 bias 파라미터도 LoRA로 튜닝할지 여부               | \"none\"(권장), \"all\"                                                                                                    |\n",
    "| **target\\_modules** | LoRA를 어떤 레이어(부분)에 적용할지 지정                     | \"q\\_proj\", \"v\\_proj\", \"o\\_proj\", \"up\\_proj\" 등<br>(모델 구조에 따라 다름)                                                      |\n",
    "| **task\\_type**      | LoRA가 적용될 문제 유형(파인튜닝 목적)                      | \"CAUSAL\\_LM\"(생성), \"SEQ\\_CLS\"(분류),<br>\"SEQ\\_2\\_SEQ\\_LM\"(번역/요약),<br>\"TOKEN\\_CLS\"(토큰분류),<br>\"QUESTION\\_ANSWERING\"(질의응답) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266ebe9-d3ce-4fed-956b-6a830acfe0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Peft(LoRA) 어뎁터 설정\n",
    "###############################\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,   \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2068ea4",
   "metadata": {},
   "source": [
    "### SFTConfig 설정\n",
    "- 전체 학습관련 설정\n",
    "\n",
    "### SFTConfig 주요 매개변수\n",
    "\n",
    "| 매개변수                                | 설명                                                  | 주요 예시 값/옵션                       |\n",
    "| ----------------------------------- | --------------------------------------------------- | ---------------------------------------       |\n",
    "| **output\\_dir**                     | 결과 모델/로그를 저장할 경로 또는 저장소 ID                          | `\"./results\"`                |\n",
    "| **num\\_train\\_epochs**              | 전체 데이터를 몇 번 반복 학습할지(에포크 수)                          | `3`, `5`                    |\n",
    "| **per\\_device\\_train\\_batch\\_size** | 각 GPU(디바이스)에서 한 번에 입력할 데이터 수(배치 크기)                 | `2`, `4`, `8`            |\n",
    "| **gradient\\_accumulation\\_steps**   | 여러 미니배치를 모아 한 번에 업데이트(실질 배치 크기 키우기)                 | `1`, `2`, `4`        |\n",
    "| **gradient\\_checkpointing**         | 메모리 절약 기능(필요할 때만 중간 계산값 저장)                         | `True`, `False`            |\n",
    "| **optim**                           | 최적화 알고리즘(학습 방법)                                     | `\"adamw_torch_fused\"`              |\n",
    "| **logging\\_steps**                  | 몇 step마다 로그를 출력할지                                   | `10`, `50`, `100`                   |\n",
    "| **save\\_strategy**                  | 모델 저장 방식(주기)                                        | `\"steps\"`, `\"epoch\"`                  |\n",
    "| **save\\_steps**                     | 몇 step마다 모델을 저장할지                                   | `50`, `100`                         |\n",
    "| **bf16**                            | bfloat16 연산 사용(GPU 메모리 절약)                          | `True`, `False`                      |\n",
    "| **learning\\_rate**                  | 파라미터 업데이트 속도(학습률)                                   | `1e-4`, `5e-5`                   |\n",
    "| **max\\_grad\\_norm**                 | 그래디언트 클리핑 임계값(학습 안정화)                               | `0.3`, `1.0`                  |\n",
    "| **warmup\\_ratio**                   | 워밍업 단계 비율(초기 학습률 천천히 증가)                            | `0.03`, `0.1`                |\n",
    "| **lr\\_scheduler\\_type**             | 학습률 조정 방식                                           | `\"constant\"`, `\"linear\"`               |\n",
    "| **push\\_to\\_hub**                   | 학습 결과를 Hugging Face Hub로 업로드할지 여부                   | `True`, `False`                  |\n",
    "| **hub\\_model\\_id**                  | 업로드할 Hugging Face Hub 저장소 ID                        |                                        |\n",
    "| **hub\\_token**                      | Hugging Face Hub 인증 토큰 사용 여부                        | `True`                                 |\n",
    "| **remove\\_unused\\_columns**         | 학습에 안 쓰는 데이터 컬럼 자동 제거 여부                            | `True`, `False`               |\n",
    "| **dataset\\_kwargs**                 | 데이터셋 추가 옵션(딕셔너리 형태)                                 | `{\"skip_prepare_dataset\": True}` |\n",
    "| **report\\_to**                      | 학습 로그를 기록할 대상(예: wandb, tensorboard, 빈 리스트면 기록 안 함) | `None`, `[\"wandb\"]`        |\n",
    "| **max\\_seq\\_length**                | 한 입력에 허용되는 최대 토큰(단어) 수                              | `2048`, `4096`, `8192`          |\n",
    "| **label\\_names**                    | Trainer가 label로 인식할 컬럼명                             | `[\"labels\"]`                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae4257-f495-4a5a-a7e6-852455deb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model_id = \"kanana-1.5-2.1b-instruct-2505-finace_news-finetuning\"\n",
    "args = SFTConfig(\n",
    "    output_dir=model_id,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4, \n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\", \n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3, \n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    \n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"kgmyh/{model_id}\",\n",
    "    hub_token=True,\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_seq_length=max_seq_length,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc5ff1-ea33-4ef2-a1dc-a6efd97cbbe4",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99cff0-d149-4ed5-8e8d-b62642165aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=trainset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b610a8-79e8-4dfe-ab1d-fe66c5aace1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#  학습 시작\n",
    "#############################\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220591cc-a64b-417e-a799-c95591887802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 모델 저장\n",
    "#############################\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213488a-a572-4518-a099-441df4801a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#  수동으로 huggingface hub에 올리기\n",
    "#  - 모델과 토크나이저를 같이 올린다.\n",
    "#  - 학습도중 업로드되도록 했어도 토크나이저는 upload 되지 않기 때문에 수동으로 올려준다.\n",
    "####################################################################################################################\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"kanana-1.5-2.1b-instruct-2505-finace_news-finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553003cb-32b3-4163-868a-f88feab8548f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a78dac-8258-41f6-9e1a-b049ca8f0ba9",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7ab74",
   "metadata": {},
   "source": [
    "## LoRA 파인튜닝 모델 사용법\n",
    "\n",
    "- 위 과정에서 LoRA(Low-Rank Adaptation) 어댑터를 파인튜닝하여 Hugging Face Hub에 업로드한 상태\n",
    "- LoRA는 전체 모델을 학습하는 대신 작은 어댑터만 학습하는 효율적인 방법\n",
    "\n",
    "- **사용 시 필요한 구성 요소**\n",
    "    - Base Model (기본 모델)\n",
    "    - LoRA Adapter (파인튜닝된 어댑터)\n",
    "    - 이 둘을 merge해서 사용한다. \n",
    "- **사용 방법 2가지**\n",
    "    1. **개별 로드 후 병합**\n",
    "        - 기본 모델과 LoRA 어댑터를 각각 다운로드\n",
    "        - 런타임에서 두 구성 요소를 병합하여 사용\n",
    "\n",
    "    2. **병합된 모델 직접 사용**\n",
    "        - 미리 병합된 상태의 모델을 다운로드\n",
    "        - 바로 사용 가능\n",
    "    - 첫 번째 방법은 메모리 효율성과 유연성을 제공하고, 두 번째 방법은 사용 편의성을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02062bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"kakaocorp/kanana-1.5-2.1b-instruct-2505\"\n",
    "lora_model_id = \"kgmyh/kanana-1.5-2.1b-instruct-2505-finace_news-finetuning\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "llama_input_creator = InputPromptCreator(tokenizer)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = \"\\\"분위기 좀 좋아지나 했는데...\\\" 삼성전자·SK하이닉스 또 애태운다\"\n",
    "news_document = \"\"\"미 도널드 트럼프 행정부가 중국 내 반도체 공장에 미국산 장비 반입을 제한하겠다는 입장을 전달한 것으로 알려졌는데 중국에 생산 기지를 둔 삼성전자와 SK하이닉스도 긴장하고 있다. 이대로라면 새 반도체 장비를 중국으로 들여올 때마다 미 정부 승인을 받아야 하기에 중국 공장 운영에 차질을 빚을 수 있다는 걱정이 나온다.\n",
    "22일 전자업계에 따르면 미 월스트리트저널(WSJ)은 20일 미 상무부가 대표적 메모리반도체 제조사인 삼성전자·SK하이닉스와 세계 파운드리(반도체 위탁생산) 1위 기업인 대만 TSMC에 미국서 만든 반도체 제조 장비의 중국에 공급할 때 줬던 '사전 허가 면제 조치'를 철회할 수 있다고 했다고 보도했다. 삼성전자와 SK하이닉스는 입장을 내놓지 않은 채 상황을 지켜보고 있다.\n",
    "2022년 조 바이든 미국 행정부는 미국산 장비의 중국 반입을 규제하는 조치를 발표했지만 삼성전자·하이닉스·TSMC 등이 운영하던 공장에는 2023년 검증된 최종 사용자(VEU) 자격을 줘 사실상 이 규제를 무기한 유예했다. 이 때문에 삼성전자와 하이닉스는 허가 없이도 장비를 중국에 들여보냈다. 새 방침은 확정된 것은 아니고 미국 정부 내에서도 의견이 엇갈린 것으로 전해졌다.\n",
    "VEU 자격 철폐설에 정부도 \"업계 우려 전달\"\n",
    "도널드 트럼프(왼쪽) 미국 대통령과 키어 스타머 영국 총리가 16일 캐나다 앨버타주 캐내내스키스에서 열린 주요 7개국(G7) 회의 도중 양국 간 무역협정 서명문을 들고 기념촬영을 하고 있다. 캐내내스키스=로이터 연합\n",
    "도널드 트럼프(왼쪽) 미국 대통령과 키어 스타머 영국 총리가 16일 캐나다 앨버타주 캐내내스키스에서 열린 주요 7개국(G7) 회의 도중 양국 간 무역협정 서명문을 들고 기념촬영을 하고 있다. 캐내내스키스=로이터 연합뉴스\n",
    "삼성전자는 중국 시안에 낸드플래시 메모리 공장이, 쑤저우에는 후공정(패키징) 공장이 있다. SK하이닉스는 우시에서 D램 공장, 충칭에서 후공정 공장, 다롄에서 인텔로부터 인수한 낸드플래시 공장을 가동 중이다. 트럼프 정부가 최첨단 극자외선(EUV) 노광 장비의 중국 내 반입은 VEU와 상관없이 통제하고 있어 이들 공장은 최신 제품보다 범용(레거시) 반도체 공급에 활용된다.\n",
    "업계에서는 실제 규제가 생기면 당장은 아니더라도 장기적으로는 공장 운영에 차질을 빚을 수 있다고 본다. 한 관계자는 \"공정을 점검하고 고장이 나면 새 장비를 들여야 한다\"면서 \"중국 공장 운영에 불확실성이 생기는 것\"이라고 설명했다.\n",
    "여한구 산업자원부 통상교섭본부장은 22일 미국 측과 관세 협상을 위해 출국을 앞두고 \"미 상무부와 무역대표부, 백악관 쪽과 접촉해 우리 업계의 우려 사항을 충분히 전달할 것\"이라며 \"건설적으로 협의해 나갈 부분이 있는지 최대한 신경 쓰겠다\"고 말했다.\"\"\"\n",
    "\n",
    "message = llama_input_creator.create_pipeline_prompt(news_title=news_title, news_document=news_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipe(message, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Merge된 모델 한번에 받아오기\n",
    "#####################################\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(lora_model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "pipe2 = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = pipe2(message, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c054923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1cadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  기반 모델로 테스트\n",
    "####################################\n",
    "\n",
    "base_pipe = pipeline(task='text-generation', model=base_model, tokenizer=tokenizer)\n",
    "response_base = base_pipe(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b655537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_base[0]['generated_text'][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c155f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c859d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
